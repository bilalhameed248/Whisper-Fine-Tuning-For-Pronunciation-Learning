{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c84d6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from datasets import Audio\n",
    "from collections import OrderedDict\n",
    "import evaluate\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8f1ba0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (C:/Users/Bilal/.cache/huggingface/datasets/audiofolder/default-218721f8d98bdfb8/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 4\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./audio_folder/\", split=\"train\")\n",
    "train_test = dataset.train_test_split(test_size=0.22)\n",
    "common_voice = DatasetDict(train_test)\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c450113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2e234744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\the.mp3\n",
      "Sentence:  the\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\the2.mp3\n",
      "Sentence:  the\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\nent.mp3\n",
      "Sentence:  nent\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\an.mp3\n",
      "Sentence:  an\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\e.mp3\n",
      "Sentence:  e\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\ti.mp3\n",
      "Sentence:  ti\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\adult.mp3\n",
      "Sentence:  adult\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\ant.mp3\n",
      "Sentence:  ant\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\wa.mp3\n",
      "Sentence:  wa\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\tica.mp3\n",
      "Sentence:  tica\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\antarctica.mp3\n",
      "Sentence:  antarctica\n",
      "\n",
      "\n",
      "\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\con1.mp3\n",
      "Sentence:  con\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\re.mp3\n",
      "Sentence:  re\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\a.mp3\n",
      "Sentence:  a\n",
      "Audio:  C:\\Users\\Bilal\\Desktop\\whisper_fine_tuning\\audio_folder\\tar.mp3\n",
      "Sentence:  tar\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(common_voice[\"train\"])):\n",
    "    print(\"Audio: \",common_voice[\"train\"][i]['audio']['path'])\n",
    "    print(\"Sentence: \",common_voice[\"train\"][i]['sentence'])\n",
    "print(\"\\n\\n\")\n",
    "for i in range(len(common_voice[\"test\"])):\n",
    "     print(\"Audio: \",common_voice[\"test\"][i]['audio']['path'])\n",
    "     print(\"Sentence: \",common_voice[\"test\"][i]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350375a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afd0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d88ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0ca06ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")\n",
    "tokenizer.save_pretrained(\"tokenizer/\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "11e6a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 the\n",
      "Decoded w/ special:    <|startoftranscript|><|en|><|transcribe|><|notimestamps|>the<|endoftext|>\n",
      "Decoded w/out s>pecial: the\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out s>pecial: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd085d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb8247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4881404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\the.mp3', 'array': array([ 0.00000000e+00,  8.60226387e-13, -9.18156610e-13, ...,\n",
      "       -2.04577600e-06,  1.10641349e-06,  5.81748964e-07]), 'sampling_rate': 24000}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0]['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9513234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "20d7b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\the.mp3', 'array': array([-1.56319402e-13,  2.84217094e-13,  1.13686838e-13, ...,\n",
      "       -7.70985935e-07, -1.47377091e-06,  1.06200332e-06]), 'sampling_rate': 16000}, 'sentence': 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8a7157d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    print(audio)\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f98e63b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\the.mp3', 'array': array([-1.56319402e-13,  2.84217094e-13,  1.13686838e-13, ...,\n",
      "       -7.70985935e-07, -1.47377091e-06,  1.06200332e-06]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\the2.mp3', 'array': array([ 0.00000000e+00, -1.09139364e-11, -9.54969437e-12, ...,\n",
      "        2.51437086e-05, -6.18215199e-06, -7.33240631e-06]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\nent.mp3', 'array': array([-9.09494702e-12,  1.00044417e-11,  2.91038305e-11, ...,\n",
      "       -1.11434929e-05, -2.98958621e-05, -3.25626497e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\an.mp3', 'array': array([-3.63797881e-12,  4.54747351e-13,  2.95585778e-12, ...,\n",
      "        2.35709595e-05,  3.83985825e-05,  1.86654361e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\e.mp3', 'array': array([ 2.72848411e-12, -2.72848411e-12, -1.27329258e-11, ...,\n",
      "        6.72080205e-07, -2.85528222e-05,  2.95111386e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\ti.mp3', 'array': array([-8.52651283e-13, -1.81898940e-12,  1.13686838e-13, ...,\n",
      "       -3.96944233e-06,  7.99322152e-06,  1.96297769e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\adult.mp3', 'array': array([ 4.54747351e-13,  3.18323146e-11, -1.09139364e-11, ...,\n",
      "       -1.00031166e-05, -1.15693058e-06, -4.07110201e-06]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\ant.mp3', 'array': array([-1.04591891e-11,  6.36646291e-12,  9.09494702e-13, ...,\n",
      "       -1.17749878e-05,  3.30894982e-05,  1.71257125e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\wa.mp3', 'array': array([ 2.72848411e-12, -9.09494702e-13, -2.54658516e-11, ...,\n",
      "        3.80619713e-06,  8.09153426e-06,  5.45018884e-06]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\tica.mp3', 'array': array([1.81898940e-12, 1.81898940e-12, 0.00000000e+00, ...,\n",
      "       1.44813384e-05, 2.12855666e-05, 2.99465173e-05]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\antarctica.mp3', 'array': array([ 7.27595761e-12,  3.45607987e-11, -9.09494702e-12, ...,\n",
      "        7.37419214e-07,  9.52164737e-06,  3.59075352e-06]), 'sampling_rate': 16000}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\con1.mp3', 'array': array([ 1.88720151e-11, -1.90993887e-11,  7.27595761e-12, ...,\n",
      "       -8.10189249e-10, -3.25185567e-09, -1.59863589e-09]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\re.mp3', 'array': array([ 2.72848411e-12, -1.90993887e-11, -3.27418093e-11, ...,\n",
      "        3.24988036e-10,  3.03941761e-09,  7.12759629e-10]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\a.mp3', 'array': array([ 3.63797881e-12,  1.04591891e-11, -5.45696821e-12, ...,\n",
      "        9.17507350e-08,  2.17551133e-08,  2.34365416e-08]), 'sampling_rate': 16000}\n",
      "{'path': 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\whisper_fine_tuning\\\\audio_folder\\\\tar.mp3', 'array': array([-2.27373675e-12, -4.54747351e-13,  6.82121026e-12, ...,\n",
      "       -1.60617856e-05, -1.02037557e-05, -1.72864111e-05]), 'sampling_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fd2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a1548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "120bc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "84fc7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d4a83c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2cc037a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "629e0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4a4183af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0453c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "82f94e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-base-pron\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=100,\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cf2c5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f58c4b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\Anaconda3\\envs\\rasa\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 2:00:18, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.3305477619171144, metrics={'train_runtime': 7280.8357, 'train_samples_per_second': 0.22, 'train_steps_per_second': 0.014, 'total_flos': 7.1345995776e+16, 'train_loss': 2.3305477619171144, 'epoch': 100.0})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2f5ca2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ee347346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline object at 0x000001C458C69850>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained('./tokenizer/', language=\"english\", task=\"transcribe\")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-base-pron\")\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"./whisper-base-pron\", tokenizer=tokenizer)\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f2e7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\Anaconda3\\envs\\rasa\\Lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "C:\\Users\\Bilal\\Anaconda3\\envs\\rasa\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Pronunciation\",\n",
    "    description=\"Realtime demo for English speech recognition using a fine-tuned Whisper base model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269ea80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b98254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eefc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7699a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47aa79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea56d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38f746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c3d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909ae3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebdba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f0fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2e4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024023c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfaed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7698633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a60906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13945b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593228e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208e47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512da33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1eea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef71b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
